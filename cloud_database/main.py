import os

from fastapi import FastAPI
import uvicorn
import requests
from datetime import datetime, timedelta
import pandas as pd

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

basetime = ['0200', '0500', '0800', '1100', '1400', '1700', '2000', '2300']
url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst'
service_key = 'cnFWOksdH2rQuZ9YQs2IR3frMjm2kgy8eauRY4ujdTSTvGEeDGXulTzCIJtU7htSZeFnoof4l6RGh3EpVIbo1Q=='  # ì¸ì¦í‚¤ (URL Encode í•„ìš” ì—†ìŒ)
base_time = '0200'
nx = 37.5606111111111  # ì˜ˆë³´ì§€ì  X ì¢Œí‘œ
ny = 127.039  # ì˜ˆë³´ì§€ì  Y ì¢Œí‘œ

app = FastAPI()
scheduler = BackgroundScheduler()

def calculate_base_time():
    """í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ base_time ë°˜í™˜"""
    now = datetime.now()
    current_hour = now.hour

    base_times = ['0200', '0500', '0800', '1100', '1400', '1700', '2000', '2300']
    base_hours = [2, 5, 8, 11, 14, 17, 20, 23]

    # í˜„ì¬ ì‹œê°ë³´ë‹¤ ì´ì „ì¸ ë°œí‘œ ì‹œê°ë“¤ ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒ
    for i in range(len(base_hours) - 1, -1, -1):  # ë’¤ì—ì„œë¶€í„° ê²€ìƒ‰
        if current_hour >= base_hours[i]:
            base_date = datetime.today().strftime('%Y%m%d')
            return base_date, base_times[i]


    if current_hour < 2:
        # í˜„ì¬ ì‹œê°ì´ 02:00ë³´ë‹¤ ì´ë¥´ë©´ ì „ë‚  23:00
        base_date = (now - timedelta(days=1)).strftime('%Y%m%d')
        return base_date, '2300'

@app.get("/")
def main():
    return {"message": "Hello World"}

@app.get("/ultra_short_data")
def get_ultra_short_data(nx, ny, base_date, base_time):
    # ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„±
    df_final = pd.DataFrame()  # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
    for i in range(1,2):
        params = {
            'serviceKey': service_key,
            'numOfRows': '1000',
            'pageNo': i,
            'dataType': 'JSON',  # JSON ë˜ëŠ” XML
            'base_date': base_date,
            'base_time': base_time,
            'nx': nx,
            'ny': ny
        }

        # ìš”ì²­ ë° ì‘ë‹µ ì²˜ë¦¬
        try:
            response = requests.get(url, params=params)
            if response.status_code == 200:
                data = response.json()  # JSON ì‘ë‹µ íŒŒì‹±
                print(data)
                result_json = data['response']['body']['items']['item']
                result_df = pd.DataFrame(result_json)
                df_final = pd.concat([df_final, result_df], ignore_index=True)  # ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
            else:
                return ["ìš”ì²­ ì‹¤íŒ¨:", response.status_code]
        except Exception as e:
            print("âŒ ìš”ì²­ ì‹¤íŒ¨:", e)

    return df_final.to_json(force_ascii=False)  # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    df_final.to_csv('ultra_short_data.csv')  # CSV íŒŒì¼ë¡œ ì €ì¥

@app.get("/short_term_data")
def get_short_term_data():
    base_date, base_time = calculate_base_time()
    params = {
        'serviceKey': service_key,  # ì¸ì¦í‚¤ (URL ì¸ì½”ë”© ì•ˆí•´ë„ ë¨)
        'numOfRows': '50',  # í•œ í˜ì´ì§€ ê²°ê³¼ ìˆ˜
        'pageNo': '1',  # í˜ì´ì§€ ë²ˆí˜¸
        'dataType': 'JSON',  # ì‘ë‹µ í˜•ì‹ (JSON or XML)
        'base_date': base_date,  # ë°œí‘œì¼ì (YYYYMMDD)
        'base_time': base_time,  # ë°œí‘œì‹œê° (HHMM)
        'nx': nx,  # ì˜ˆë³´ì§€ì  X ì¢Œí‘œ
        'ny': ny  # ì˜ˆë³´ì§€ì  Y ì¢Œí‘œ
    }

    response = requests.get(url, params=params)

    if response.status_code == 200:
        data = response.json()

    else:
        print("âŒ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.get("/scheduler/status")
def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸"""
    jobs = []
    for job in scheduler.get_jobs():
        jobs.append({
            "id": job.id,
            "name": job.name,
            "next_run": str(job.next_run_time),
            "trigger": str(job.trigger)
        })
    return {
        "running": scheduler.running,
        "jobs": jobs
    }

def download_ultra_short_data():
    print("ğŸ»ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    os.makedirs("data", exist_ok=True)  # ë°ì´í„° ì €ì¥ í´ë” ìƒì„±
    region_code_df = pd.read_csv('ì§€ì—­_ì½”ë“œ_ì •ë¦¬.csv', encoding='utf-8-sig')
    base_date, base_time = calculate_base_time()
    now_year = str(datetime.now().year)
    now_month = str(datetime.now().month)

    os.makedirs(os.path.join('data', now_year), exist_ok=True)  # ë°ì´í„° ì €ì¥ í´ë” ìƒì„±
    if os.path.exists(os.path.join('data', now_year, f"{now_year}_{now_month}.csv")):
        check_region_df = pd.read_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), encoding='utf-8-sig')

        for index, row in region_code_df.iterrows():
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ì X'], row['ê²©ì Y']
            region_df_check = region_code_df[(region_code_df['ê²©ì X'] == nx & region_code_df['ê²©ì Y'] == ny
                                              & region_code_df['baseTime'] == base_time & region_code_df[
                                                  'baseDate'] == base_date)]
            if region_df_check != 835 or region_df_check != 943:
                data = pd.read_json(get_ultra_short_data(nx, ny, base_date, base_time), orient='records')
                data = data[data['category'] == 'SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
                data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
                data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

                df_all_region = pd.concat([df_all_region, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°

    else:
        for index, row in region_code_df.iterrows():
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ì X'], row['ê²©ì Y']
            data = pd.read_json(get_ultra_short_data(nx, ny, base_date, base_time), orient='records')
            data = data[data['category']=='SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
            data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4)) # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
            data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4)) # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

            df_all_region = pd.concat([df_all_region, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°


    if not os.path.exists(os.path.join('data', now_year, f"{now_year}_{now_month}.csv")):
        df_all_region.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), header=False, index=False)
    else:
        data_final = pd.read_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"))
        data_final = pd.concat([data_final, df_all_region], ignore_index=True)
        data_final.drop_duplicates(inplace=True)
        data_final.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), mode='a', header=False,
                          index=False)
    print(f"ğŸ»âœ…ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - {base_date} {base_time} ê¸°ì¤€")


scheduler.add_job(
    func=download_ultra_short_data,
    trigger=CronTrigger(minute=11),  # ë§¤ì‹œê°„ 10ë¶„ì— ì‹¤í–‰
    id='weather_job',
    name='Weather API Job'
)

# FastAPI ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@app.on_event("startup")
async def startup_event():
    scheduler.start()  # ğŸ”¥ ì¤‘ìš”: ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘!
    print("ğŸ“… ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("â° ë§¤ì‹œê°„ 10ë¶„ë§ˆë‹¤ ê¸°ìƒ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

@app.on_event("shutdown")
async def shutdown_event():
    scheduler.shutdown()
    print("ğŸ“… ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    uvicorn.run("main:app", host='0.0.0.0', reload=True, port=8000)