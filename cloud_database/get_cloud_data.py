import os
import sys
from datetime import datetime, timedelta
import pandas as pd
from tqdm import tqdm
from io import StringIO
from zoneinfo import ZoneInfo
import requests
import logging

# ì„¤ì • ìƒìˆ˜ë“¤
BASE_TIMES = ['0200', '0500', '0800', '1100', '1400', '1700', '2000', '2300']
BASE_HOURS = [2, 5, 8, 11, 14, 17, 20, 23]
URL = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst'
SEOUL_TZ = ZoneInfo("Asia/Seoul")

SERVICE_KEY = os.getenv("API_KEY")


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class WeatherDataCollector:
    def __init__(self, region_csv_path='ì§€ì—­_ì½”ë“œ_ì •ë¦¬.csv'):
        self.region_df = pd.read_csv(region_csv_path, encoding='utf-8-sig')
        self.now = datetime.now(SEOUL_TZ)
        self.now_year = str(self.now.year)
        self.now_month = str(self.now.month)
        self.data_dir = os.path.join('data', self.now_year)
        os.makedirs(self.data_dir, exist_ok=True)

    def _calculate_base_time_for_short_term(self):
        """ë‹¨ê¸°ì˜ˆë³´ìš© base_time ê³„ì‚° (02,05,08,11,14,17,20,23ì‹œ)"""
        current_hour = self.now.hour

        # í˜„ì¬ ì‹œê°ë³´ë‹¤ ì´ì „ì¸ ë°œí‘œ ì‹œê°ë“¤ ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒ
        for i in range(len(BASE_HOURS) - 1, -1, -1):
            if current_hour >= BASE_HOURS[i]:
                return self.now.strftime('%Y%m%d'), BASE_TIMES[i]

        # í˜„ì¬ ì‹œê°ì´ 02:00ë³´ë‹¤ ì´ë¥´ë©´ ì „ë‚  23:00
        yesterday = self.now - timedelta(days=1)
        return yesterday.strftime('%Y%m%d'), '2300'

    def _calculate_base_time_for_ultra_short(self):
        """ì´ˆë‹¨ê¸°ì˜ˆë³´ìš© base_time ê³„ì‚° (ë§¤ì‹œê°„ 10ë¶„ ë°œí‘œ, ì •ê° base_time)"""
        current_hour = self.now.hour
        current_minute = self.now.minute

        # í˜„ì¬ ì‹œê°„ì´ 10ë¶„ ì´í›„ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©, ì•„ë‹ˆë©´ ì´ì „ ì‹œê°„ ì‚¬ìš©
        if current_minute >= 10:
            base_hour = current_hour
            base_date = self.now.strftime('%Y%m%d')
        else:
            if current_hour == 0:
                # 00ì‹œ 10ë¶„ ì´ì „ì´ë©´ ì „ë‚  23ì‹œ ê¸°ì¤€
                yesterday = self.now - timedelta(days=1)
                base_date = yesterday.strftime('%Y%m%d')
                base_hour = 23
            else:
                base_hour = current_hour - 1
                base_date = self.now.strftime('%Y%m%d')

        base_time = f"{base_hour:02d}00"
        return base_date, base_time

    def _make_api_request(self, nx, ny, base_date, base_time, num_rows=1000):
        """API ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ì²˜ë¦¬"""
        params = {
            'serviceKey': SERVICE_KEY,
            'numOfRows': str(num_rows),
            'pageNo': '1',
            'dataType': 'JSON',
            'base_date': base_date,
            'base_time': base_time,
            'nx': nx,
            'ny': ny
        }

        try:
            response = requests.get(URL, params=params, timeout=30)
            if response.status_code == 200:
                data = response.json()
                items = data['response']['body']['items']['item']
                return pd.DataFrame(items)
            else:
                logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} for nx={nx}, ny={ny}")
                return pd.DataFrame()
        except requests.exceptions.RequestException as e:
            logger.error(f"ìš”ì²­ ì‹¤íŒ¨ (nx={nx}, ny={ny}): {e}")
            return pd.DataFrame()
        except (KeyError, ValueError) as e:
            logger.error(f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ (nx={nx}, ny={ny}): {e}")
            return pd.DataFrame()

    def _process_weather_data(self, df):
        """ê¸°ìƒ ë°ì´í„° ì „ì²˜ë¦¬"""
        if df.empty:
            return df

        # SKY ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
        df = df[df['category'] == 'SKY'].copy()

        # ì‹œê°„ í˜•ì‹ í†µì¼
        df['baseTime'] = df['baseTime'].astype(str).str.zfill(4)
        df['fcstTime'] = df['fcstTime'].astype(str).str.zfill(4)

        return df.reset_index(drop=True)

    def _get_existing_data(self, file_path):
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(file_path):
            return pd.read_csv(file_path, encoding='utf-8-sig')
        return pd.DataFrame()

    def _check_data_completeness(self, existing_df, nx, ny, longitude, latitude, base_date, base_time, data_type):
        """íŠ¹ì • ì§€ì—­ì˜ ë°ì´í„° ì™„ì„±ë„ í™•ì¸"""
        if existing_df.empty:
            return False

        # ê¸°ë³¸ í•„í„°ë§ ì¡°ê±´ (nx, ny, baseTime, baseDateë¡œë§Œ í™•ì¸)
        # longitude, latitudeëŠ” ë‚˜ì¤‘ì— ì¶”ê°€ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„í„°ë§ì—ì„œ ì œì™¸
        region_data = existing_df[
            (existing_df['nx'] == nx) &
            (existing_df['ny'] == ny) &
            (existing_df['baseTime'] == base_time) &
            (existing_df['baseDate'] == int(base_date))
            ]

        # ë°ì´í„° ì™„ì„±ë„ í™•ì¸
        if data_type == 'ultra_short':
            # ì´ˆë‹¨ê¸°ì˜ˆë³´ëŠ” 6ì‹œê°„(6ê°œ ì‹œê°„ëŒ€) ì˜ˆë³´
            expected_records = 6
        else:
            # ë‹¨ê¸°ì˜ˆë³´ëŠ” 3ì¼ê°„ ì˜ˆë³´ (72ì‹œê°„)
            expected_records = 72

        return len(region_data) >= expected_records

    def _should_collect_data(self, data_type):
        """ë°ì´í„° ìˆ˜ì§‘ ì—¬ë¶€ íŒë‹¨"""
        if data_type == 'ultra_short':
            # ì´ˆë‹¨ê¸°ì˜ˆë³´: ë§¤ì‹œê°„ 30ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
            return True
        else:
            # ë‹¨ê¸°ì˜ˆë³´: base ì‹œê°„(02,05,08,11,14,17,20,23ì‹œ)ì—ë§Œ ìˆ˜ì§‘
            current_hour = self.now.hour
            return current_hour in BASE_HOURS

    def collect_weather_data(self, data_type='ultra_short'):
        """ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜"""
        # ìˆ˜ì§‘ ì‹œì  í™•ì¸
        if not self._should_collect_data(data_type):
            logger.info(f"â° {data_type} ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return

        # base_time ê³„ì‚°
        if data_type == 'ultra_short':
            base_date, base_time = self._calculate_base_time_for_ultra_short()
        else:
            base_date, base_time = self._calculate_base_time_for_short_term()

        logger.info(f"ğŸ» {data_type} ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ - {base_date} {base_time} ê¸°ì¤€")

        file_suffix = '_ultra' if data_type == 'ultra_short' else '_short_term'
        file_path = os.path.join(self.data_dir, f"{self.now_year}_{self.now_month}{file_suffix}.csv")

        existing_df = self._get_existing_data(file_path)
        new_data_list = []
        skipped_count = 0

        for _, row in tqdm(self.region_df.iterrows(),
                           total=len(self.region_df),
                           desc=f"ğŸŒ¤ï¸ {data_type} ê¸°ìƒ ë°ì´í„° ì²˜ë¦¬ ì¤‘"):

            nx, ny = row['ê²©ì X'], row['ê²©ì Y']
            longitude, latitude = row['ê²½ë„(ì´ˆ/100)'], row['ìœ„ë„(ì´ˆ/100)']

            # ê¸°ì¡´ ë°ì´í„°ê°€ ì™„ì „í•œì§€ í™•ì¸
            if self._check_data_completeness(existing_df, nx, ny, longitude, latitude, base_date, base_time, data_type):
                skipped_count += 1
                continue

            # ìƒˆ ë°ì´í„° ìˆ˜ì§‘
            raw_data = self._make_api_request(nx, ny, base_date, base_time)
            processed_data = self._process_weather_data(raw_data)

            if not processed_data.empty:
                new_data_list.append(processed_data)

        logger.info(f"ğŸ“Š ìŠ¤í‚µëœ ì§€ì—­: {skipped_count}, ìƒˆë¡œ ìˆ˜ì§‘ëœ ì§€ì—­: {len(new_data_list)}")

        # ë°ì´í„° ë³‘í•© ë° ì €ì¥
        if new_data_list:
            self._save_data(existing_df, new_data_list, file_path)
        else:
            logger.info("ğŸ’¡ ìˆ˜ì§‘í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        logger.info(f"ğŸ»âœ… {data_type} ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - {base_date} {base_time} ê¸°ì¤€")

    def _add_location_to_existing_data(self, df, column_type):
        """ê¸°ì¡´ ë°ì´í„°ì— ìœ„ì¹˜ ì •ë³´ ì¶”ê°€"""
        if df.empty:
            return df

        # ë³µì‚¬ë³¸ ìƒì„±
        df_copy = df.copy()

        # ì§€ì—­ ì •ë³´ì™€ ë§¤ì¹­í•´ì„œ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
        for _, region_row in self.region_df.iterrows():
            nx, ny = region_row['ê²©ì X'], region_row['ê²©ì Y']

            if column_type == 'longitude':
                location_value = region_row['ê²½ë„(ì´ˆ/100)']
            else:  # latitude
                location_value = region_row['ìœ„ë„(ì´ˆ/100)']

            # í•´ë‹¹ nx, nyì— í•´ë‹¹í•˜ëŠ” í–‰ë“¤ì— ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
            mask = (df_copy['nx'] == nx) & (df_copy['ny'] == ny)
            df_copy.loc[mask, column_type] = location_value

        return df_copy

    def _save_data(self, existing_df, new_data_list, file_path):
        """ë°ì´í„° ì €ì¥ - ëˆ„ë½ëœ ì»¬ëŸ¼ ìë™ ì¶”ê°€"""
        logger.info("ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")

        new_df = pd.concat(new_data_list, ignore_index=True)

        if not existing_df.empty:
            # ê¸°ì¡´ ë°ì´í„°ì— longitude, latitude ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if 'longitude' not in existing_df.columns:
                logger.info("ğŸ“ ê¸°ì¡´ ë°ì´í„°ì— longitude ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
                existing_df = self._add_location_to_existing_data(existing_df, 'longitude')

            if 'latitude' not in existing_df.columns:
                logger.info("ğŸ“ ê¸°ì¡´ ë°ì´í„°ì— latitude ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
                existing_df = self._add_location_to_existing_data(existing_df, 'latitude')

            final_df = pd.concat([existing_df, new_df], ignore_index=True)
            final_df.drop_duplicates(inplace=True)
        else:
            final_df = new_df

        final_df.to_csv(file_path, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {len(final_df)} ë ˆì½”ë“œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        collector = WeatherDataCollector()

        # ultra_shortì™€ short_term ë‘˜ ë‹¤ ìˆ˜ì§‘
        for data_type in ['ultra_short', 'short_term']:
            collector.collect_weather_data(data_type)

    except Exception as e:
        logger.error(f"âŒ ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
